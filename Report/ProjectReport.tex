%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University Assignment Title Page 
% LaTeX Template
% Version 1.0 (27/12/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% WikiBooks (http://en.wikibooks.org/wiki/LaTeX/Title_Creation)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
% Instructions for using this template:
% This title page is capable of being compiled as is. This is not useful for 
% including it in another document. To do this, you have two options: 
%
% 1) Copy/paste everything between \begin{document} and \end{document} 
% starting at \begin{titlepage} and paste this into another LaTeX file where you 
% want your title page.
% OR
% 2) Remove everything outside the \begin{titlepage} and \end{titlepage} and 
% move this file to the same directory as the LaTeX file you wish to add it to. 
% Then add \input{./title_page_1.tex} to your LaTeX file where you want your
% title page.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\title{Title page with logo}
%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{float}

\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE Università degli studi di Milano-Bicocca}\\[1cm] % Name of your university/college
\textsc{\Large Advanced Machine Learning }\\[0.3cm] % Major heading such as course name
\textsc{\large Final Project}\\[0.1cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries PetFinder.my - Pawpularity Contest}\\[0.4cm] % Title of your document
{ \large \textbf{Predizione Della Popolarità Di Foto Di Animali}}
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\large
\emph{Authors:}\\
Francesco Lenti - 865274 - f.lenti3@campus.unimib.it \\   % Your name
Mattia Boller - 873358 - m.boller@campus.unimib.it   \\
Mattia Marchi - 817587 - m.marchi@campus.unimib.it   \\[1cm] % Your name

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

\includegraphics{logo.png}\\[1cm] % Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}


\begin{abstract}
    Una immagine vale più di mille parole. Sapevi che un' immagine può salvare più di mille vite? Milioni di animali randagi soffrono per le strade o vengono soppressi nei rifugi ogni giorno in tutto il mondo. È chiaro aspettarsi che gli animali con foto più "attraenti" generino più interesse e vengano adottati più velocemente. Ma cosa rende "attraente" una immagine? Con l'aiuto del Deep Learning cercheremo di determinare l'attrattiva di una foto di un animale al fine di dargli una maggiore possibilità di adozione. In questo documento verranno illustrate le tecniche e le soluzioni adottate, tramite l'utilizzo di diversi modelli di deep neural network, per ottenere tale score di popolarità. 
\end{abstract}

\section{Introduzione}
    Il task fa riferimento ad una competizione aperta su Kaggle.
    La competizione è finanziata da PetFinder.my, principale piattaforma per il benessere degli animali della Malesia. Attualmente, PetFinder.my utilizza un misuratore per stimare la popolarità delle foto di animali randagi ospitati all'interno di rifugi. Questo misuratore analizza le statistiche riguardanti il traffico generato dalle foto degli animali, presenti su diversi portali, e ne stima la popolarità. Sebbene questo strumento sia utile, potrebbe essere ancora migliorato. L'obiettivo finale è quello di stimare a priori la "Pawpularity" (termine coniato dalla piattaforma per indicare la popolarità di una foto) di un animale in base alla foto del suo profilo, in modo di scegliere quelle che ne faciliterebbero l'adozione. Insieme alle foto di migliaia di animali vengono anche forniti dei metadati, etichettati a mano, per ogni foto. Vengono forniti quindi due dataset, uno con le foto degli animali e l'altro con i corrispettivi metadati.
    La soluzione da noi proposta utilizza un modello multi-input, per utilizzare tutte le informazioni a disposizione. Da un lato una rete convoluzionale che sfrutta un approccio di fine-tuning basato su transfer learning, dall'altro una semplice rete neurale fully-connected. Gli output delle due reti vengono concatenati e processati da un ulteriore layer fully-connected finale per ottenere l'output, ovvero il Pawpularity score.


\section{Datasets}
    Vengono forniti un dataset di immagini e un dataset tabulare contenente i metadati delle immagini.\\ In particolare, i dati di training comprendono:
    \begin{itemize}
        \item Una cartella \textbf{train} contenente 9912 immagini di training nella forma \textit{\{id\}.jpg}, dove id è un identificativo univoco del profilo dell'animale;
        \item Un file \textbf{\textit{train.csv}} contenente i metadati e il Pawpularity score per ogni immagine.
    \end{itemize}

    I dati di testing non sono disponibili, dato che si tratta di una competizione Kaggle ancora aperta al momento della stesura di questo report. Tuttavia, vengono fornite la cartella \textbf{test} e il file \textbf{\textit{test.csv}} di 8 immagini al fine di testare la submission dell'algoritmo.
    Inoltre è presente anche un file \textbf{\textit{sample\_submission.csv}} con un esempio di submission. Dopo aver inviato la submission, il modello verrà testato su un subset pari al 25\% del dataset originale di test, contenete circa 6800 immagini, il quale verrà utilizzato solo al termine della competizione per stilare la classifica finale.

    
    \subsection{Data Exploration}
    Processo per investigare quelle che sono le caratteristiche chiave dei dataset. Prima di costruire un modello di machine learning è necessario capire il dataset con cui si sta lavorando e il problema che si sta cercando di risolvere. Entriamo ora nel dettaglio.

    \subsubsection{Immagini}
    Per quanto riguarda il dataset delle immagini andremo a mostrare alcune foto in base al pawpularity score assegnato.
    Analizziamo quindi le immagini partendo dai voti più bassi andando verso quelli più alti. Per non sporcare troppo il report sono stati inseriti solo 3 gradi di pawpularity: 1, 50 e 100, il resto dell'analisi è disponibile nel notebook.
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.5]{Plot/pawpularity_1.jpg}
        \caption{Animali con Pawpularity=1}
        \label{fig:paw_10}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.5]{Plot/pawpularity_50.jpg}
        \caption{Animali con Pawpularity=50}
        \label{fig:paw_50}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.5]{Plot/pawpularity_100.jpg}
        \caption{Animali con Pawpularity=100}
        \label{fig:paw_100}
    \end{figure}

    Dall'analisi visiva effettuata non si evincono particolari dettagli su cosa possa rendere una foto più "popolare" rispetto ad un'altra, è facile, infatti, trovare delle belle foto di animali con score di pawpularity sotto il 10.

    \subsubsection{Metadati}
    Come descritto in precedenza il file \textit{train.csv} contiene i metadati per ogni immagine. In figura \ref{fig:target} visualizziamo la distribuzione del target, ovvero il Pawpularity score.
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.8]{Plot/distribution_target.jpg}
        \caption{Distribuzione e esempio della feature Focus}
        \label{fig:target}
    \end{figure}

    Osserviamo come la distribuzione dello score sia concentrata principalmente tra i valori 20 e 50. È facile osservare anche una forte presenza di immagini con score uguale a 100.
    Successivamente analizziamo la distribuzione del target per ogni singola feature del dataset.
    Per fare ciò usiamo degli istogrammi, plottiamo il Pawpularity score sull'asse x e la somma degli 0s e degli 1s di ogni feature sull'asse y. Questo ci può aiutare a visualizzare se alcune feature impattano più di altre nel calcolo del Pawpularity score. Per ogni feature inoltre vengono mostrati degli esempi di immagini per contestualizzare meglio il significato della feature stessa.

    \begin{itemize}
        \item \textbf{Focus} - L'animale è posizionato su uno sfondo ordinato, non troppo vicino/lontano;
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{Plot/distribution_subjectfocus.jpg}
            \caption{Distribuzione e esempio della feature Focus}
            \label{fig:focus}
        \end{figure}
        \item \textbf{Eyes} - Entrambi gli occhi sono rivolti verso l'obiettivo, con almeno un occhio/pupilla decentemente chiaro;
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{Plot/distribution_eyes.jpg}
            \caption{Distribuzione e esempio della feature Eyes}
            \label{fig:eyes}
        \end{figure}
        \item \textbf{Face} - Viso discretamente chiaro e rivolto in avanti;
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{Plot/distribution_face.jpg}
            \caption{Distribuzione e esempio della feature Face}
            \label{fig:face}
        \end{figure}
        \item \textbf{Near} - Singolo animale che occupa una porzione significativa della foto (circa oltre il 50\% della larghezza o dell'altezza della foto);
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{Plot/distribution_near.jpg}
            \caption{Distribuzione e esempio della feature Near}
            \label{fig:near}
        \end{figure}
        \item \textbf{Action} - Animale nel mentre di un'azione (ad es. saltare);
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{Plot/distribution_action.jpg}
            \caption{Distribuzione e esempio della feature Action}
            \label{fig:action}
        \end{figure}
        \item \textbf{Accessory} - Accessorio/oggetto di accompagnamento fisico o digitale, come un giocattolo o un adesivo digitale, esclusi collare e guinzaglio;
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{Plot/distribution_accessory.jpg}
            \caption{Distribuzione e esempio della feature Accessory}
            \label{fig:accessory}
        \end{figure}
        \item \textbf{Group} - Più di 1 animale nella foto;
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{Plot/distribution_group.jpg}
            \caption{Distribuzione e esempio della feature Group}
            \label{fig:group}
        \end{figure}
        \item \textbf{Collage} - Foto ritoccata digitalmente;
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{Plot/distribution_collage.jpg}
            \caption{Distribuzione e esempio della feature Collage}
            \label{fig:collage}
        \end{figure}
        \item \textbf{Human} - Umano nella foto;
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{Plot/distribution_human.jpg}
            \caption{Distribuzione e esempio della feature Human}
            \label{fig:human}
        \end{figure}
        \item \textbf{Occlusion} - Oggetti indesiderati che bloccano una parte dell'animale. Nota: Non tutti gli oggetti bloccanti sono considerati occlusione;
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{Plot/distribution_occlusion.jpg}
            \caption{Distribuzione e esempio della feature Occlusion}
            \label{fig:occlusion}
        \end{figure}
        \item \textbf{Info} - Mesto o etichette personalizzati (ad es. nome dell'animale, descrizione);
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{Plot/distribution_info.jpg}
            \caption{Distribuzione e esempio della feature Info}
            \label{fig:info}
        \end{figure}
        \item \textbf{Blur} - Notevolmente sfocato o rumoroso, soprattutto per gli occhi e il viso dell'animale. Per le voci Sfocatura, la colonna "Occhi" è sempre impostata su 0.
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{Plot/distribution_blur.jpg}
            \caption{Distribuzione e esempio della feature Blur}
            \label{fig:blur}
        \end{figure}
    \end{itemize}
    
    Intuitivamente possiamo dire che questi grafici non ci dicono molto. La distribuzione del Pawpularity score è simile per ogni feature, in altre parole nessuna feature in particolare sembra influenzare particolarmente il Pawpularity score.
    \vfill

\section{The Methodological Approach}

    La competizione proposta da PetFinder.my si presenta come un problema di regressione, con l'obiettivo di creare un modello capace di assegnare un punteggio di attrattività, 
    compreso tra 0 e 100, ad una determinata foto di un animale.
    Nei seguenti paragrafi verrano presentati i vari step che hanno portato alla definizione del modello finale utilizzato per il task sopra descritto.
    Si è partiti da alcuni modelli di deep learning più tradizionali, quali fully-connected neural networks e convolutional neural networks, per poi cercare di combinare le informazioni
    contenute nelle immagini con le informazioni date dai metadati allegati alle foto.
    Si anticipa che l'approccio finale proposto è basato su un modello multi-input single-output. La motivazione alla base di questo approccio è quella di sviluppare un sistema di 
    apprendimento che sia in grado di sfruttare le informazioni utili da features miste di dati, poiché questi tipi di dati di solito richiedono un trattamento separato. 
    A tal fine, ogni tipo di dato in ingresso viene elaborato e gestito in modo diverso e indipendente.

    \subsection{Convolutional neural network}
    \label{cnn}
        Il primo approccio adottato per cercare di risolvere il problema e per dare una base di partenza da cui poi definire modelli più complessi, è stato quello di sfruttare
        una convolutional neural network, classico tipo di rete neurale adatto a trattare immagini. Con questo primo tentativo si è cercato di capire che performance era possibile
        ottenere sfruttando solamente le foto, lasciando momentaneamente in disparte i metadati forniti.

        Il modello costruito si basa su transfer learning e fine-tuning di una rete allenata in precedenza, in modo da poter sfruttare le elevate performance di questi modelli andando
        ad abbattere i tempi di allenamento da zero di una nuova rete di grandezza simile. In particolare, per il task trattato da questo caso di studio, si è optato per la rete 
        EfficientNet-B3. Le EfficientNet sono una famiglia di reti presentate nel maggio 2019, basate su un metodo di model scaling innovativo rispetto ad esempio a MobileNet o ResNet
        il quale va a scalare uniformemente le dimensioni di larghezza, profondità e risoluzione utilizzando un coefficiente composto $\phi$ nella seguente maniera:
        \begin{equation}
            \begin{split}
                depth:&\ d = \alpha^\phi\\
                width:&\ w = \beta^\phi\\
                resolution:&\  r = \gamma^\phi\\
                s.t.&\ \alpha \cdot \beta^2 \cdot \gamma^2 \approx 2\\
                &\ \alpha\geq1, \beta\geq1, \gamma\geq1
            \end{split}
        \end{equation}
        dove $\alpha$, $\beta$, $\gamma$ sono costanti determinabili da una piccola grid search.
        
        La versione B7 di EfficientNet ha raggiunto una top-1 accuracy su ImageNet pari al 84.3\%, raggiungendo lo stato dell'arte a parità di Gpipe pur essendo 8.4 volte più piccola e 6.1 volte più veloce nell'inferenza.
        EfficientNet si è inoltre dimostrata molto performante in caso di utilizzo in transfer learning, in particolare raggiungendo un'accuracy pari al 91.7\% su Cifar-100 e al 98.8\% 
        su Flowers. Nel caso del problema trattato in questo documento, si è scelta la versione B3, la quale prende in input immagini di dimensione 300x300, decisione dettata dal trade-off tra compattezza
        (e quindi limiti hardware) e performance della rete. 
        Le performance delle varie versioni di EfficientNet su ImageNet rispetto ad altre reti sono mostrate in figura \ref{fig:efficientnet}.

        \begin{figure}[h!]
            \centering
            \includegraphics[scale=0.2]{Plot/efficientnet.png}
            \caption{Performance di EfficientNet rispetto ad altre reti.}
            \label{fig:efficientnet}
        \end{figure}
        
        Inoltre, i pesi importati, sono quelli ottenuti tramite la tecnica di training Noisy Student. Noisy Student è una metodo di apprendimento semi-supervisionato che si basa sull'allenare
        un modello (teacher) con immagini etichettate, utilizzarlo per etichettare nuove immagini e allenare un nuovo modello (student), della stessa grandezza o maggiore, con le immagini iniziali e 
        quelle nuove ottenute dal primo modello aggiugendo del rumore. Questi passi vengono poi ripetuti più volte, invertendo teacher e student. Attraverso questa tecnica, utilizzando
        300M di immagini non etichettate, EfficientNet-B7 ha raggiunto una top-1 accuracy su ImageNet pari al 88.4\%.

        In coda alla rete pre-allenata importata, sono stati aggiunti dei layer fully-connected per adattare il modello al problema di regressione di PetFinder.my. Di seguito
        è specificata l'architettura della sezione aggiunta:
        \begin{itemize}
            \item \textbf{Dense} - 1024 neuroni, ReLU
            \item \textbf{Dropout} - 50\% dei neuroni eliminati.
            \item \textbf{Dense} - 512 neuroni, ReLU
            \item \textbf{Dropout} - 30\% dei neuroni eliminati.
            \item \textbf{Dense} - 128 neuroni, ReLU
            \item \textbf{Dropout} - 10\% dei neuroni eliminati.
            \item \textbf{Dense} - 1 neurone, Linear
        \end{itemize}
        La funzione di loss scelta è stata la Mean Squared Error, mentre per monitorare le performance è stata utilizzata come metrica la Root Mean Squared Error, così da 
        avere un'idea sulla reale grandezza dell'errore. Come algoritmo di ottimizzazione è stato selezionato Adam.

    \subsection{Fully-connected neural network}
    \label{mlp}
    Per trattare i metadati riguardati le foto, si è deciso di utilizzare una rete neurale classica fully-connected. Di seguito viene 
    riportata l'architettura del modello:
    \begin{itemize}
        \item \textbf{Dense} - 32 neuroni, ReLU
        \item \textbf{Dropout} - 20\% dei neuroni eliminati.
        \item \textbf{Dense} - 8 neuroni, ReLU
        \item \textbf{Dropout} - 10\% dei neuroni eliminati.
        \item \textbf{Dense} - 1 neurone, Linear
    \end{itemize}
    Anche in questo caso, come per la rete convoluzionale, si sono utilizzate MSE come funzione di loss, RMSE come metrica delle performance del modello e Adam come algoritmo di ottimizzazione.

    \subsection{Multi-input \& mixed data neural network}
    \label{mulinput}
    L'ipotesi secondo cui predizioni più accurate si possono ottenere combinando le informazioni derivanti dalle foto degli animali e dai metadati ad esse allegati, ha portato alla
    costruzione di un terzo modello, combinazione dei due precedentemente descritti.

    Il modello in questione si presenta come una rete neurale multi-input, costituita dalla rete convoluzionale illustrata nel paragrafo \ref{cnn} e dalla rete fully-connected di cui si
    indicano le specifiche nel paragrafo \ref{mlp}, i quali output vengono concatenati e processati da due ulteriori layer.
    L'architettura proposta è mostrata in figura \ref{fig:model}.
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.35]{Plot/Model-Plot.png}
            \caption{Architettura Multi-input Single-output Mixed-data}
            \label{fig:model}
        \end{figure}

    \subsection{Training}
    Il training del modello finale descritto nel paragrafo \ref{mulinput}, è avvenuto allenando prima separatamente i due modelli di cui è composto. Entrambe le reti sono state 
    allenate utilizzando una grandezza di batch di 32, per un massimo di 15 epoche, applicando in entrambi i casi early-stopping per evitare casi di overfitting. Di seguito, 
    in figura \ref{fig:loss_cnn} e in figura \ref{fig:loss_mlp} sono presentati i grafici del valore di loss durante i training della rete convoluzionale e della rete fully-connected.
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{Plot/loss-cnn.png}
            \caption{Grafico del valore di loss durante il training della rete CNN}
            \label{fig:loss_cnn}
        \end{figure}
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{Plot/loss-mlp.png}
            \caption{Grafico del valore di loss durante il training della rete Fully-Connected}
            \label{fig:loss_mlp}
        \end{figure}
    I due modelli allenati sono stati successivamente uniti per andare a formare la rete multi-input e i loro pesi sono stati bloccati, così da poter effettuare il training 
    solo sulla sezione aggiunta in seguito alla concatenzaione dei risultati.

    Durante la fase di training del terzo modello, si è riscontrato un fenomeno di blocco in un minimo locale, il quale impediva miglioramenti delle performance. Il problema
    è stato affrontato effettuando uno studio sul valore ideale di learning rate, ricerca che ha portato alle scelta di un valore pari a 0.1, più alto rispetto ai valori inizialmente
    adottati. In figura \ref{fig:learning_rate} viene presentato il grafico dello studio che ha portato alla scelta del particolare valore di learning rate.
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{Plot/learning-rate.png}
            \caption{Grafico del valore di loss rispetto a diversi valori di learning rate}
            \label{fig:learning_rate}
        \end{figure}

    Il modello multi-input è stato allenato con grandezza di batch pari a 32, per un massimo di 30 epoche, anche in questo caso limitate da early-stopping. In figura \ref{fig:loss_mulinput} è
    presentato il grafico del valore di loss durante il training del modello finale.
        \begin{figure}[h!]
            \centering
            \includegraphics[scale=0.5]{Plot/loss-final.png}
            \caption{Grafico del valore di loss durante il training della rete Multi-input}
            \label{fig:loss_mulinput}
        \end{figure}




\newpage


\section{Risultati e Valutazione}

La metrica usata per la valutazione delle performance, essendo un task di regressione, è rappresentato dal calcolo del \textbf{RMSE (Root Mean Square Error)}:
\[
\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} 
\]
con $\hat{y}_i$ il valore di popolarità predetto e $y_i$ il valore originale.


Tale metrica è stata scelta perché, grazie all'elevazione al quadrato delle deviazioni, evita che i risultanti valori positivi e negativi possano annullarsi a vicenda. Le performance vengono valutate nelle varie epoche, sia sul training set che sul validation set. Tale valutazione viene effettuata prima sui singoli modelli, figure \ref{fig:rmse-cnn} e \ref{fig:rmse-mlp}, e successivamente, in figura \ref{fig:rmse-final}, sul modello finale ottenuto tramite la concatenazione dei primi due. E' possibile notare come nella rete finale non ci sia traccia di overfitting, a differenza del modello ottenuto dalla rete convoluzionale in cui, sul train set, lo score continua a decrescere senza migliorare sul validation set. 


    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.6]{Plot/CNN_RMSE.png}
        \caption{RMSE del modello Efficient-Net B3 su train set (blu) e validation set (arancione) durante le epoche.}
        \label{fig:rmse-cnn}
    \end{figure}

\newpage


    \begin{figure}[h]
        \centering
        \vspace*{-2.5cm}
        \includegraphics[scale=0.55]{Plot/MLP_RMSE.png}
        \caption{RMSE del modello Multilayer Perceptron su train set (blu) e validation set (arancione) durante le epoche.}
        \label{fig:rmse-mlp}
    \end{figure}
    

    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.55]{Plot/FINAL_RMSE.png}
        \caption{RMSE del Modello Finale su train set (blu) e validation set (arancione) durante le epoche.}
        \label{fig:rmse-final}
    \end{figure}
    
\newpage


\subsection{Confronto Risultati}


Nella tabella \ref{table:rmse} vengono raffigurati i valori migliori ottenuti sul validation set per le varie architetture. La figura \ref{fig:rmse-all} mostra l'andamento della metrica RMSE dei differenti modelli nelle varie epoche.

\vspace{1cm}

\begin{table}[h]
\caption{Confronto RMSE ottenuta tra i diversi modelli sul validation set.}
  \vspace{3mm}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Modello}   & \textbf{RMSE Validation} \\ \hline \hline
Efficient-Net B3      & 1               \\ \hline
MLP       & 1               \\ \hline
Efficient-Net B3  + MLP & 1               \\ \hline
\end{tabular}
\label{table:rmse}
\end{table}

\vspace{1cm}

\begin{figure}[h]
    \centering
    
    \includegraphics[scale=0.6]{Plot/ALL_RMSE.png}
    \caption{RMSE modelli a confronto.}
    \label{fig:rmse-all}
\end{figure}





\newpage


\subsection{Grad CAM}

Un problema noto del deep learning è rappresentato dal fatto che i modelli creati siano "black box", ovvero che sia difficilmente spiegabile come realmente le informazioni siano trattate all'interno dei vari layers. Per analizzare i risultati ottenuti è stato ritenuto opportuno effettuare un ulteriore studio su come la rete convoluzionale ragionasse sulle immagini per ottenere lo score di popolarità. Questo è possibile utilizzando lo strumento \textbf{Grad CAM\cite{2019}}. Tale metodo consente di produrre una spiegazione visiva di ciò che la rete osserva, determinando quali sono gli aspetti e le features che più influenzano lo score finale. E' possibile notare nella figura \ref{fig:grad-cam} come la rete si vada a focalizzare maggiormente sull'animale nel caso in cui l'immagine risulti pulita da oggetti di disturbo; al contrario la rete andrà a dare maggior importanza all'ambiente circostante piuttosto che al solo animale.

\vspace{1cm}

\begin{figure}[h]
        \centering
        \includegraphics[scale=0.6]{Plot/GRAD-CAM.png}
        \caption{Confronto tra due Grad CAM ottenute da immagini con diversi score: quella di sinistra con popolarità più alta, mentre quella di destra più bassa.}
        \label{fig:grad-cam}
\end{figure}
    

    
    
\newpage


\section{Considerazioni}

I risultati mostrano come le performance ottenute dal modello finale siano migliori di quelle ottenute tramite il solo fine-tuning sulla rete Efficient-Net B3 e nettamente migliori di quelli del Multilayer Perceptron sui meta-dati. Questo è dovuto da un utilizzo di tutte le informazioni disponibili e non solo le immagini: il layer fully-connected del modello finale riesce a dare i giusti pesi alle informazioni ottenute dalla rete convoluzionale, per le immagini, e da quella fully-connected, per i meta-dati, migliorando il risultato finale. Le performance ottenute risultano migliori di quelle raggiunte da altri team kaggle utilizzanti reti convoluzionali allo stato dell'arte. I migliori risultati tuttavia vengono ottenuti da team che sfruttando un altro tipo di modello, ovvero i \textbf{transformer}, migliorando lo score finale di un punto rispetto all'utilizzo di reti convoluzionali.


\section{Conclusions}
Conclusions should summarize the central points made in the Discussion section, reinforcing for the reader the value and implications of the work. If the results were not definitive, specific future work that may be needed can be (briefly) described. The conclusions should never contain ``surprises''. Therefore, any conclusions should be based on observations and data already discussed. It is considered extremely bad form to introduce new data in the conclusions.

\section*{References}

The references section should contain complete citations following standard form.  The references should be numbered and listed in the order they were cited in the body of the report. In the text of the report, a particular reference can be cited by using a numerical number in brackets as \cite{Lee2015} that corresponds to its number in the reference list. \LaTeX provides several styles to format the references

\bibliographystyle{IEEEtran}
\bibliography{references.bib}

\end{document}